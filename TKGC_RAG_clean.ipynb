{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## This is the main notebook of the LLM + RAG part of the project\n",
    "\n",
    "It should be possible to use it for both the filtered and full dataset as well for all major RAG versions with only small adjustments in the code (e.g. setting the arguments of eval_open_world_hits_1_3 function)"
   ],
   "id": "f8a7af22d4d3a8a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Setup",
   "id": "0c0983ef-e19e-4198-b700-041792c14ce4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hostname: node18.enst.fr\n",
      "CUDA_VISIBLE_DEVICES: 0\n",
      "/home/infres/mporwisz-25/miniconda3/envs/tkgc_rag/bin/python\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import socket, os, sys, warnings\n",
    "print(\"hostname:\", socket.gethostname())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(sys.executable)\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "ef846024-bfc1-41b0-88ad-c8f0085e63cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|███████████████████████| 398/398 [00:46<00:00,  8.49it/s, Materializing param=model.norm.weight]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# LLM: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\n",
    "llm_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# We want to use 4bit quantization to save memory (in case some of you use their own computer)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False, load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\")\n",
    "\n",
    "# Prevent some transformers specific issues.\n",
    "tokenizer.use_default_system_prompt = False\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load LLM.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0}, # load all the model layers on GPU 0\n",
    "    torch_dtype=torch.bfloat16, # float precision\n",
    ")\n",
    "\n",
    "# Set LLM on eval mode.\n",
    "llm.eval()"
   ],
   "id": "cf02689e-367c-4312-8010-3110710d4a5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "from transformers import set_seed\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "id": "b589f4ab-0ddd-46ff-aa1f-637337e20eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "import json\n",
    "\n",
    "qid_to_label = json.load(open(\"qid_to_label.json\"))\n",
    "pid_to_label = json.load(open(\"pid_to_label.json\"))\n",
    "norm_label_to_qids = json.load(open(\"norm_label_to_qids.json\"))"
   ],
   "id": "e87de05b-d1d9-428a-98d4-1fc7e8e9a202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468790 81586\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "# load and split dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tkgl-smallpedia_edgelist.csv\")\n",
    "df[\"ts\"] = df[\"ts\"].astype(int)\n",
    "\n",
    "train_df = df[df[\"ts\"] < 2008].copy()\n",
    "test_df  = df[df[\"ts\"] >= 2008].copy()\n",
    "\n",
    "print(len(train_df), len(test_df))"
   ],
   "id": "bc88690f-12df-4a0c-a24e-511e1feea117"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # for filtering data\n",
    "# train_df_filter = df[df[\"ts\"] < 1998].copy()\n",
    "#\n",
    "# train_entities = set(pd.concat([train_df_filter[\"head\"], train_df_filter[\"tail\"]]).astype(str).unique())\n",
    "#\n",
    "# test_df_filtered = test_df[\n",
    "#     (test_df[\"head\"].astype(str).isin(train_entities)) &\n",
    "#     (test_df[\"tail\"].astype(str).isin(train_entities))\n",
    "# ].copy()\n",
    "#\n",
    "# test_df_filtered.drop_duplicates()\n",
    "# len(test_df_filtered)"
   ],
   "id": "5bcf9dae21c73a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. RAG",
   "id": "14625b12-45d9-4446-8cc5-3025fdb0cc12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_hr_index(df):\n",
    "    idx = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        h = str(row[\"head\"])\n",
    "        r = str(row[\"relation_type\"])\n",
    "        t = str(row[\"tail\"])\n",
    "        ts = int(row[\"ts\"])\n",
    "        idx[(h, r)].append((ts, t))\n",
    "    # sort by ts so it's easy to retreive close entries\n",
    "    for k in idx:\n",
    "        idx[k].sort(key=lambda x: x[0])\n",
    "    return idx\n",
    "\n",
    "hr_index = build_hr_index(train_df)"
   ],
   "id": "9b9d4126-5d83-48fd-a18d-c12178184ec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "def build_head_index(df):\n",
    "    idx = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        h = str(row[\"head\"])\n",
    "        r = str(row[\"relation_type\"])\n",
    "        t = str(row[\"tail\"])\n",
    "        ts = int(row[\"ts\"])\n",
    "        idx[h].append((ts, r, t))\n",
    "    # sort by ts so it's easy to retreive close entries\n",
    "    for k in idx:\n",
    "        idx[k].sort(key=lambda x: x[0])\n",
    "    return idx\n",
    "\n",
    "head_index = build_head_index(train_df)"
   ],
   "id": "da1e4e73-7c83-4366-920f-b23b2f4bc5d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "def build_entity_index(df):\n",
    "    idx = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        h = str(row[\"head\"])\n",
    "        r = str(row[\"relation_type\"])\n",
    "        t = str(row[\"tail\"])\n",
    "        ts = int(row[\"ts\"])\n",
    "\n",
    "        idx[h].append((ts, h, r, t))\n",
    "        idx[t].append((ts, h, r, t))\n",
    "\n",
    "    for e in idx:\n",
    "        idx[e].sort(key=lambda x: x[0])\n",
    "    return idx\n",
    "\n",
    "entity_index = build_entity_index(train_df)\n"
   ],
   "id": "da035813-8625-48aa-98c1-9d165672c05d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "def build_rel_index(df):\n",
    "    idx = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        ts = int(row[\"ts\"])\n",
    "        h = str(row[\"head\"])\n",
    "        r = str(row[\"relation_type\"])\n",
    "        t = str(row[\"tail\"])\n",
    "        idx[r].append((ts, h, t))\n",
    "    for r in idx:\n",
    "        idx[r].sort(key=lambda x: x[0])\n",
    "    return idx\n",
    "\n",
    "rel_index = build_rel_index(train_df)"
   ],
   "id": "610aee45-aa6c-4376-8f38-2c7fd9c30707"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "def retrieve_facts_new(head_id, rel_id, ts, k=12):\n",
    "    out = []\n",
    "\n",
    "    # 1) exact match: (head, rel) -> (ts, h, r, t)\n",
    "    facts_hr = hr_index.get((head_id, rel_id), [])  # list[(ts, tail_id)]\n",
    "    if facts_hr:\n",
    "        ranked = sorted(facts_hr, key=lambda x: abs(x[0] - ts))\n",
    "        out.extend([(y, head_id, rel_id, tail_id) for (y, tail_id) in ranked])\n",
    "\n",
    "    # 2) fallback: (head, *) -> already (ts, r, t) OR (ts, r, tail_id)\n",
    "    if len(out) < k:\n",
    "        facts_h = head_index.get(head_id, [])  # list[(ts, r, tail_id)]\n",
    "        if facts_h:\n",
    "            ranked_h = sorted(facts_h, key=lambda x: abs(x[0] - ts))\n",
    "            out.extend([(y, head_id, r, tail_id) for (y, r, tail_id) in ranked_h])\n",
    "\n",
    "    # 3) fallback: facts about entity (as head OR tail)\n",
    "    if len(out) < k:\n",
    "        facts_e = entity_index.get(head_id, [])  # list[(ts, h, r, t)]\n",
    "        if facts_e:\n",
    "            ranked_e = sorted(facts_e, key=lambda x: abs(x[0] - ts))\n",
    "            # dodaj tylko tyle ile brakuje\n",
    "            need = k - len(out)\n",
    "            out.extend(ranked_e[:need])\n",
    "\n",
    "    # 4) relation-only examples (global)\n",
    "    if len(out) < k:\n",
    "        facts_r = rel_index.get(rel_id, [])  # [(ts, h, t)]\n",
    "        ranked_r = sorted(facts_r, key=lambda x: abs(x[0] - ts))\n",
    "        need = k - len(out)\n",
    "        out.extend([(y, h, rel_id, t) for (y, h, t) in ranked_r[:need]])\n",
    "\n",
    "    # filter to k\n",
    "    out = out[:k]\n",
    "\n",
    "    # change to text\n",
    "    facts_txt = []\n",
    "    for y, h, r, t in out:\n",
    "        h_label = qid_to_label.get(h, h)\n",
    "        r_label = pid_to_label.get(r, r)\n",
    "        t_label = qid_to_label.get(t, t)\n",
    "        facts_txt.append(f\"In {y}, {h_label} {r_label} {t_label}.\")\n",
    "\n",
    "    return facts_txt"
   ],
   "id": "6c144364-136d-4a72-b106-a13adff3412a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "def normalize_label(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"\\.$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def tail_label_to_qids(tail_label: str):\n",
    "    n = normalize_label(tail_label)\n",
    "    return norm_label_to_qids.get(n, [])\n"
   ],
   "id": "84360d11-1ad6-41a3-8407-243e46ffd015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 16,
   "source": [
    "# Define the function to call the qwen model\n",
    "def qwen_llm_candidates(prompt):\n",
    "    # 1. Instruction prompt\n",
    "    system_prompt = (\n",
    "        \"You solve Temporal Knowledge Graph Completion (TKGC).\\n\"\n",
    "        \"You will be given a query (head, relation, timestamp), retrieved facts, and a candidate list.\\n\"\n",
    "        \"Your task is to select the most likely tail entity from the candidate list.\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1) Choose up to 3 candidates ONLY from the provided candidate list.\\n\"\n",
    "        \"2) Order them from best to worst.\\n\"\n",
    "        \"3) Return ONLY valid JSON in ONE line, and nothing else.\\n\"\n",
    "        '4) Output format: {\"tail_labels\": [\"...\", \"...\", \"...\"]}\\n'\n",
    "        '   If no candidate matches, return {\"tail_labels\": []}.\\n'\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "        ).to(llm.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = llm.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "\n",
    "    raw = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip()\n",
    "    return raw"
   ],
   "id": "3983c121-1997-4a0b-adb2-d46de7d727dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You solve Temporal Knowledge Graph Completion (open-world).\\n\"\n",
    "    \"Given a query (head, relation, timestamp) and retrieved facts, predict the most likely tail entity.\\n\"\n",
    "    \"The correct tail may NOT appear in the retrieved facts.\\n\"\n",
    "    \"Return up to 3 tail entity labels ordered best→worst.\\n\"\n",
    "    \"Return ONLY one-line valid JSON and nothing else.\\n\"\n",
    "    'Format: {\"tail_labels\": [\"...\",\"...\",\"...\"]}\\n'\n",
    "    'If you cannot propose any, return {\"tail_labels\": []}.'\n",
    ")\n"
   ],
   "id": "a68c5616-6b60-4a6f-b723-dd6431af43a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": [
    "# Define the function to call the qwen model\n",
    "def qwen_llm_open_top3(prompt):\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You solve Temporal Knowledge Graph Completion (open-world).\\n\"\n",
    "        \"Given a query (head, relation, timestamp) and retrieved facts, predict the most likely tail entity.\\n\"\n",
    "        \"The correct tail may NOT appear in the retrieved facts.\\n\"\n",
    "        \"Return up to 3 tail entity labels ordered best→worst.\\n\"\n",
    "        \"Return ONLY one-line valid JSON and nothing else.\\n\"\n",
    "        'Format: {\"tail_labels\": [\"...\",\"...\",\"...\"]}\\n'\n",
    "        'If you cannot propose any, return {\"tail_labels\": []}.'\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "        ).to(llm.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = llm.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "\n",
    "    raw = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip()\n",
    "    return raw"
   ],
   "id": "46e1f3ce-7498-456e-857b-f75dc35043f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "def build_user_prompt_candidates_top3(head_label, rel_label, ts, retrieved_facts, candidates):\n",
    "    facts_block = \"\\n\".join([f\"- {f}\" for f in retrieved_facts])\n",
    "    cand_block = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(candidates)])\n",
    "\n",
    "    return (\n",
    "        f\"Query:\\nTime: {ts}\\nHead: {head_label}\\nRelation: {rel_label}\\n\\n\"\n",
    "        f\"Retrieved facts:\\n{facts_block}\\n\\n\"\n",
    "        f\"Candidates (pick up to 3):\\n{cand_block}\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1) Choose up to 3 candidates ONLY from the candidate list.\\n\"\n",
    "        \"2) Order them best→worst.\\n\"\n",
    "        '3) Return ONLY JSON: {\"tail_labels\": [\"...\",\"...\",\"...\"]}.\\n'\n",
    "        '4) If none match, return {\"tail_labels\": []}.'\n",
    "    )"
   ],
   "id": "9321d41a-0226-4bfe-8756-f07d0e0469a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 20,
   "source": [
    "def rag_answer_candidates_top3(head_id: str, rel_id: str, ts: int, k=12):\n",
    "    head_label = qid_to_label.get(head_id, head_id)\n",
    "    rel_label  = pid_to_label.get(rel_id, rel_id)\n",
    "\n",
    "    retrieved_facts = retrieve_facts_new(head_id, rel_id, ts, k=k)\n",
    "    if not retrieved_facts:\n",
    "        return [], [], '{\"tail_labels\": []}'\n",
    "\n",
    "    candidates = extract_tail_candidates(retrieved_facts, head_label, rel_label)\n",
    "    if not candidates:\n",
    "        return [], retrieved_facts, '{\"tail_labels\": []}'\n",
    "\n",
    "    user_prompt = build_user_prompt_candidates_top3(head_label, rel_label, ts, retrieved_facts, candidates)\n",
    "    raw = qwen_llm_candidates(user_prompt)\n",
    "    pred_labels = extract_tail_labels_topk(raw, k=3) or []\n",
    "    return pred_labels, retrieved_facts, raw"
   ],
   "id": "1b3902cb-b8a5-4b24-89cb-5f6a45dd3630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 21,
   "source": [
    "def extract_tail_candidates(retrieved_facts: list[str], head_label: str, rel_label: str):\n",
    "    \"\"\"\n",
    "    Returns unique tail labels (strings) extracted from facts in format:\n",
    "      In YEAR, HEAD REL TAIL.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    key = f\"{head_label} {rel_label} \"\n",
    "\n",
    "    for f in retrieved_facts:\n",
    "        if key not in f:\n",
    "            continue\n",
    "\n",
    "        tail = f.split(key, 1)[1].strip()\n",
    "\n",
    "        # delete dot at the end\n",
    "        tail = re.sub(r\"\\.\\s*$\", \"\", tail)\n",
    "\n",
    "        tail = tail.strip()\n",
    "\n",
    "        if tail:\n",
    "            candidates.append(tail)\n",
    "\n",
    "    # unique candidates with retained order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            uniq.append(c)\n",
    "    return uniq\n"
   ],
   "id": "ddc91cd1-7fb0-481c-8029-e692447e6ee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 54,
   "source": [
    "def rag_answer_open_world_top3(head_id: str, rel_id: str, ts: int, k=24):\n",
    "    head_label = qid_to_label.get(head_id, head_id)\n",
    "    rel_label  = pid_to_label.get(rel_id, rel_id)\n",
    "\n",
    "    retrieved_facts = retrieve_facts_new(head_id, rel_id, ts, k=k)\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Query:\\nTime: {ts}\\nHead: {head_label}\\nRelation: {rel_label}\\n\\n\"\n",
    "        \"Retrieved facts:\\n\" + \"\\n\".join([f\"- {f}\" for f in retrieved_facts]) + \"\\n\\n\"\n",
    "        \"Task:\\n\"\n",
    "        \"Predict up to 3 most likely tail entity labels (best→worst).\\n\"\n",
    "        \"The correct tail may NOT be in the retrieved facts.\\n\"\n",
    "        'Return ONLY JSON: {\"tail_labels\": [\"...\",\"...\",\"...\"]}\\n'\n",
    "        'If none, return {\"tail_labels\": []}.'\n",
    "    )\n",
    "\n",
    "    raw = qwen_llm_open_top3(user_prompt)\n",
    "    pred_labels = extract_tail_labels_topk(raw, k=3)  # None / [] / [\"a\",\"b\",\"c\"]\n",
    "    return pred_labels, retrieved_facts, raw\n"
   ],
   "id": "fbc12aff-20b2-4fc4-98c5-15765ffdc00f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 46,
   "source": [
    "def rag_answer_hybrid_top3(head_id: str, rel_id: str, ts: int, k=24, rag_threshold=1):\n",
    "    # try candidate-only\n",
    "    head_label = qid_to_label.get(head_id, head_id)\n",
    "    rel_label  = pid_to_label.get(rel_id, rel_id)\n",
    "\n",
    "    retrieved_facts = retrieve_facts_new(head_id, rel_id, ts, k=k)\n",
    "\n",
    "    candidates = extract_tail_candidates(retrieved_facts, head_label, rel_label)\n",
    "\n",
    "    if len(candidates) >= rag_threshold:\n",
    "        # candidate-only top3\n",
    "        pred_labels, retrieved_facts2, raw = rag_answer_candidates_top3(head_id, rel_id, ts, k=k)\n",
    "        return pred_labels, retrieved_facts2, raw, \"candidates\"\n",
    "\n",
    "    # fallback: open-world top3\n",
    "    pred_labels, retrieved_facts2, raw = rag_answer_open_world_top3(head_id, rel_id, ts, k=k)\n",
    "    return pred_labels, retrieved_facts2, raw, \"open_world\"\n"
   ],
   "id": "9fa748b2-9dfe-4bba-ad55-5e6383720987"
  },
  {
   "cell_type": "markdown",
   "id": "2ad2fef9-7134-442a-a33b-1b849ed1224f",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1427743a-ae34-4c3c-8e29-5c54067bfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
    "\n",
    "def extract_tail_labels_topk(raw: str, k=3):\n",
    "    if not raw:\n",
    "        return None\n",
    "\n",
    "    cleaned = raw.strip()\n",
    "    cleaned = re.sub(r\"^```(?:json)?\", \"\", cleaned, flags=re.IGNORECASE).strip()\n",
    "    cleaned = re.sub(r\"```$\", \"\", cleaned).strip()\n",
    "\n",
    "    m = _JSON_RE.search(cleaned)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    block = m.group(0)\n",
    "    try:\n",
    "        obj = json.loads(block)\n",
    "    except json.JSONDecodeError: \n",
    "        # just in case for apostrophes\n",
    "        try:\n",
    "            obj = json.loads(block.replace(\"'\", '\"'))\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    labels = obj.get(\"tail_labels\", None)\n",
    "    if labels is None:\n",
    "        return None\n",
    "    if not isinstance(labels, list):\n",
    "        return None\n",
    "\n",
    "    labels = [x.strip() for x in labels if isinstance(x, str) and x.strip()]\n",
    "    return labels[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e315833-534f-41db-8df0-d3afebd086c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_in_retrieved(gold_tail_id: str, retrieved_facts: list[str]) -> bool:\n",
    "    gold_label = qid_to_label.get(gold_tail_id, gold_tail_id)\n",
    "    gl = gold_label.lower()\n",
    "    return any(gl in f.lower() for f in retrieved_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd619cd5-a607-4512-966d-d5953079efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_open_world_hits_1_3(df, rag_answer, max_examples=None, log_every=100, rag_threshold=1):\n",
    "    h1 = 0\n",
    "    h3 = 0\n",
    "    total = 0\n",
    "    parse_fail = 0\n",
    "    empty = 0\n",
    "    no_map = 0\n",
    "\n",
    "    rag_cand = 0\n",
    "    rag_open = 0\n",
    "    recall_hits = 0\n",
    "    cand_h1 = 0\n",
    "    cand_h3 = 0\n",
    "    open_h1 = 0\n",
    "    open_h3 = 0\n",
    "\n",
    "    N = len(df) if max_examples is None else min(len(df), max_examples)\n",
    "\n",
    "    for i in range(N):\n",
    "        row = df.iloc[i]\n",
    "        ts = int(row[\"ts\"])\n",
    "        head_id = str(row[\"head\"])\n",
    "        rel_id  = str(row[\"relation_type\"])\n",
    "        gold_tail = str(row[\"tail\"])\n",
    "\n",
    "        answer_ver = None\n",
    "\n",
    "        if rag_answer == rag_answer_hybrid_top3:\n",
    "            pred_labels, retrieved_facts, raw, answer_ver = rag_answer(head_id, rel_id, ts, k=24, rag_threshold=rag_threshold)\n",
    "    \n",
    "            if answer_ver == \"candidates\":\n",
    "                rag_cand += 1\n",
    "                # recall@k\n",
    "                if gold_in_retrieved(gold_tail, retrieved_facts):\n",
    "                    recall_hits += 1\n",
    "    \n",
    "            if answer_ver == \"open_world\":\n",
    "                rag_open += 1\n",
    "        else:\n",
    "            pred_labels, retrieved_facts, raw = rag_answer(head_id, rel_id, ts, k=24)\n",
    "            \n",
    "        if pred_labels is None:\n",
    "            parse_fail += 1\n",
    "            total += 1\n",
    "            continue\n",
    "        if len(pred_labels) == 0:\n",
    "            empty += 1\n",
    "            total += 1\n",
    "            continue\n",
    "\n",
    "        # mapping label -> qids\n",
    "        qids_1 = tail_label_to_qids(pred_labels[0])\n",
    "        qids_3 = set()\n",
    "        for lab in pred_labels[:3]:\n",
    "            qids_3.update(tail_label_to_qids(lab))\n",
    "\n",
    "        if (not qids_1) and (not qids_3):\n",
    "            no_map += 1\n",
    "\n",
    "        if gold_tail in qids_1:\n",
    "            h1 += 1\n",
    "            if answer_ver == \"candidates\":\n",
    "                cand_h1 += 1\n",
    "            if answer_ver == \"open_world\":\n",
    "                open_h1 += 1\n",
    "\n",
    "            \n",
    "        if gold_tail in qids_3:\n",
    "            h3 += 1\n",
    "            if answer_ver == \"candidates\":\n",
    "                cand_h3 += 1\n",
    "            if answer_ver == \"open_world\":\n",
    "                open_h3 += 1\n",
    "\n",
    "        total += 1\n",
    "        if log_every and (i+1) % log_every == 0:\n",
    "            print(f\"{i+1}/{N} H@1={h1/total:.3f} H@3={h3/total:.3f} parse_fail={parse_fail} empty={empty} no_map={no_map}\")\n",
    "\n",
    "    return {\"hits@1\": h1/total, \"hits@3\": h3/total, \"total\": total, \"parse_fail\": parse_fail, \n",
    "            \"empty\": empty, \"no_map\": no_map, \n",
    "            \"rag_candidates\": rag_cand, \"rag_open_world\": rag_open,\n",
    "            \"hits@1_candidates\": cand_h1 / max(rag_cand, 1), \"hits@1_open_world\": open_h1 / max(rag_open, 1),\n",
    "            \"hits@3_candidates\": cand_h3 / max(rag_cand, 1), \"hits@3_open_world\": open_h3 / max(rag_open, 1),\n",
    "            \"recall@k\": recall_hits / max(rag_cand, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f452a85-608a-4857-9671-c27982b7d5d7",
   "metadata": {},
   "source": [
    "### Comparison tests on 500 sample - hybrid with different rag switch thresholds (1, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27e0ce75-e7e7-4e4f-a1e5-df5426ff6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = test_df.sample(n=500, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2373c39-3d78-4947-a340-28e0bb6150d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/500 H@1=0.450 H@3=0.500 parse_fail=0 empty=0 no_map=0\n",
      "40/500 H@1=0.375 H@3=0.425 parse_fail=0 empty=0 no_map=0\n",
      "60/500 H@1=0.367 H@3=0.433 parse_fail=0 empty=0 no_map=0\n",
      "80/500 H@1=0.312 H@3=0.400 parse_fail=0 empty=0 no_map=1\n",
      "100/500 H@1=0.330 H@3=0.400 parse_fail=0 empty=0 no_map=2\n",
      "120/500 H@1=0.342 H@3=0.417 parse_fail=0 empty=0 no_map=2\n",
      "140/500 H@1=0.329 H@3=0.407 parse_fail=0 empty=0 no_map=2\n",
      "160/500 H@1=0.312 H@3=0.388 parse_fail=0 empty=0 no_map=5\n",
      "180/500 H@1=0.328 H@3=0.400 parse_fail=0 empty=0 no_map=5\n",
      "200/500 H@1=0.330 H@3=0.405 parse_fail=0 empty=0 no_map=5\n",
      "220/500 H@1=0.345 H@3=0.414 parse_fail=0 empty=0 no_map=8\n",
      "240/500 H@1=0.325 H@3=0.392 parse_fail=0 empty=0 no_map=9\n",
      "260/500 H@1=0.319 H@3=0.381 parse_fail=0 empty=0 no_map=9\n",
      "280/500 H@1=0.325 H@3=0.386 parse_fail=0 empty=0 no_map=10\n",
      "300/500 H@1=0.330 H@3=0.390 parse_fail=0 empty=0 no_map=10\n",
      "320/500 H@1=0.334 H@3=0.391 parse_fail=0 empty=0 no_map=10\n",
      "340/500 H@1=0.326 H@3=0.385 parse_fail=0 empty=0 no_map=10\n",
      "360/500 H@1=0.328 H@3=0.386 parse_fail=0 empty=0 no_map=10\n",
      "380/500 H@1=0.326 H@3=0.384 parse_fail=0 empty=0 no_map=13\n",
      "400/500 H@1=0.323 H@3=0.378 parse_fail=0 empty=0 no_map=14\n",
      "420/500 H@1=0.326 H@3=0.379 parse_fail=0 empty=0 no_map=15\n",
      "440/500 H@1=0.325 H@3=0.377 parse_fail=0 empty=0 no_map=15\n",
      "460/500 H@1=0.328 H@3=0.378 parse_fail=0 empty=0 no_map=15\n",
      "480/500 H@1=0.335 H@3=0.383 parse_fail=0 empty=0 no_map=15\n",
      "500/500 H@1=0.342 H@3=0.390 parse_fail=0 empty=0 no_map=15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hits@1': 0.342,\n",
       " 'hits@3': 0.39,\n",
       " 'total': 500,\n",
       " 'parse_fail': 0,\n",
       " 'empty': 0,\n",
       " 'no_map': 15,\n",
       " 'rag_candidates': 376,\n",
       " 'rag_open_world': 124,\n",
       " 'hits@1_candidates': 0.43882978723404253,\n",
       " 'hits@1_open_world': 0.04838709677419355,\n",
       " 'hits@3_candidates': 0.4946808510638298,\n",
       " 'hits@3_open_world': 0.07258064516129033,\n",
       " 'recall@k': 0.5212765957446809}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = eval_open_world_hits_1_3(df=sample_df, rag_answer=rag_answer_hybrid_top3, \n",
    "                                   max_examples=len(sample_df), log_every=20, rag_threshold=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55634dc6-c6ea-4f45-a8e0-ed0fcd1e194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/500 H@1=0.400 H@3=0.500 parse_fail=0 empty=0 no_map=0\n",
      "40/500 H@1=0.350 H@3=0.425 parse_fail=0 empty=0 no_map=0\n",
      "60/500 H@1=0.350 H@3=0.433 parse_fail=0 empty=0 no_map=0\n",
      "80/500 H@1=0.300 H@3=0.400 parse_fail=0 empty=0 no_map=1\n",
      "100/500 H@1=0.320 H@3=0.400 parse_fail=0 empty=0 no_map=2\n",
      "120/500 H@1=0.333 H@3=0.417 parse_fail=0 empty=0 no_map=2\n",
      "140/500 H@1=0.314 H@3=0.407 parse_fail=0 empty=0 no_map=2\n",
      "160/500 H@1=0.287 H@3=0.381 parse_fail=1 empty=0 no_map=5\n",
      "180/500 H@1=0.306 H@3=0.394 parse_fail=1 empty=0 no_map=5\n",
      "200/500 H@1=0.305 H@3=0.400 parse_fail=1 empty=0 no_map=5\n",
      "220/500 H@1=0.318 H@3=0.409 parse_fail=1 empty=0 no_map=8\n",
      "240/500 H@1=0.296 H@3=0.383 parse_fail=1 empty=0 no_map=9\n",
      "260/500 H@1=0.292 H@3=0.373 parse_fail=1 empty=0 no_map=9\n",
      "280/500 H@1=0.296 H@3=0.375 parse_fail=1 empty=0 no_map=11\n",
      "300/500 H@1=0.300 H@3=0.380 parse_fail=1 empty=0 no_map=11\n",
      "320/500 H@1=0.306 H@3=0.381 parse_fail=1 empty=0 no_map=11\n",
      "340/500 H@1=0.300 H@3=0.379 parse_fail=1 empty=0 no_map=11\n",
      "360/500 H@1=0.306 H@3=0.381 parse_fail=2 empty=0 no_map=11\n",
      "380/500 H@1=0.303 H@3=0.379 parse_fail=2 empty=0 no_map=14\n",
      "400/500 H@1=0.300 H@3=0.372 parse_fail=2 empty=0 no_map=15\n",
      "420/500 H@1=0.305 H@3=0.374 parse_fail=3 empty=0 no_map=16\n",
      "440/500 H@1=0.305 H@3=0.375 parse_fail=3 empty=0 no_map=16\n",
      "460/500 H@1=0.311 H@3=0.378 parse_fail=3 empty=0 no_map=16\n",
      "480/500 H@1=0.319 H@3=0.383 parse_fail=3 empty=0 no_map=17\n",
      "500/500 H@1=0.324 H@3=0.388 parse_fail=3 empty=0 no_map=18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hits@1': 0.324,\n",
       " 'hits@3': 0.388,\n",
       " 'total': 500,\n",
       " 'parse_fail': 3,\n",
       " 'empty': 0,\n",
       " 'no_map': 18,\n",
       " 'rag_candidates': 131,\n",
       " 'rag_open_world': 369,\n",
       " 'hits@1_candidates': 0.25190839694656486,\n",
       " 'hits@1_open_world': 0.34959349593495936,\n",
       " 'hits@3_candidates': 0.3816793893129771,\n",
       " 'hits@3_open_world': 0.3902439024390244,\n",
       " 'recall@k': 0.42748091603053434}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = eval_open_world_hits_1_3(df=sample_df, rag_answer=rag_answer_hybrid_top3, \n",
    "                                   max_examples=len(sample_df), log_every=20, rag_threshold=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42342c3c-0e8e-47ce-8731-d0fba02a021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/500 H@1=0.400 H@3=0.500 parse_fail=0 empty=0 no_map=0\n",
      "40/500 H@1=0.350 H@3=0.425 parse_fail=0 empty=0 no_map=0\n",
      "60/500 H@1=0.350 H@3=0.433 parse_fail=0 empty=0 no_map=0\n",
      "80/500 H@1=0.287 H@3=0.388 parse_fail=0 empty=0 no_map=1\n",
      "100/500 H@1=0.310 H@3=0.390 parse_fail=0 empty=0 no_map=2\n",
      "120/500 H@1=0.325 H@3=0.408 parse_fail=0 empty=0 no_map=2\n",
      "140/500 H@1=0.307 H@3=0.400 parse_fail=0 empty=0 no_map=2\n",
      "160/500 H@1=0.281 H@3=0.375 parse_fail=1 empty=0 no_map=5\n",
      "180/500 H@1=0.300 H@3=0.389 parse_fail=1 empty=0 no_map=5\n",
      "200/500 H@1=0.305 H@3=0.395 parse_fail=1 empty=0 no_map=5\n",
      "220/500 H@1=0.318 H@3=0.405 parse_fail=1 empty=0 no_map=8\n",
      "240/500 H@1=0.296 H@3=0.379 parse_fail=1 empty=0 no_map=9\n",
      "260/500 H@1=0.292 H@3=0.369 parse_fail=1 empty=0 no_map=9\n",
      "280/500 H@1=0.296 H@3=0.371 parse_fail=1 empty=0 no_map=11\n",
      "300/500 H@1=0.300 H@3=0.377 parse_fail=1 empty=0 no_map=11\n",
      "320/500 H@1=0.306 H@3=0.378 parse_fail=1 empty=0 no_map=11\n",
      "340/500 H@1=0.300 H@3=0.376 parse_fail=1 empty=0 no_map=11\n",
      "360/500 H@1=0.306 H@3=0.378 parse_fail=2 empty=0 no_map=11\n",
      "380/500 H@1=0.300 H@3=0.376 parse_fail=2 empty=0 no_map=14\n",
      "400/500 H@1=0.297 H@3=0.370 parse_fail=2 empty=0 no_map=15\n",
      "420/500 H@1=0.300 H@3=0.371 parse_fail=3 empty=0 no_map=16\n",
      "440/500 H@1=0.300 H@3=0.373 parse_fail=3 empty=0 no_map=16\n",
      "460/500 H@1=0.307 H@3=0.376 parse_fail=3 empty=0 no_map=16\n",
      "480/500 H@1=0.315 H@3=0.381 parse_fail=3 empty=0 no_map=17\n",
      "500/500 H@1=0.320 H@3=0.386 parse_fail=3 empty=0 no_map=18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hits@1': 0.32,\n",
       " 'hits@3': 0.386,\n",
       " 'total': 500,\n",
       " 'parse_fail': 3,\n",
       " 'empty': 0,\n",
       " 'no_map': 18,\n",
       " 'rag_candidates': 58,\n",
       " 'rag_open_world': 442,\n",
       " 'hits@1_candidates': 0.1896551724137931,\n",
       " 'hits@1_open_world': 0.33710407239819007,\n",
       " 'hits@3_candidates': 0.3103448275862069,\n",
       " 'hits@3_open_world': 0.39592760180995473,\n",
       " 'recall@k': 0.41379310344827586}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = eval_open_world_hits_1_3(df=sample_df, rag_answer=rag_answer_hybrid_top3, \n",
    "                                   max_examples=len(sample_df), log_every=20, rag_threshold=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292190bc-8487-4409-af55-9ed9eca2e595",
   "metadata": {},
   "source": [
    "### open-world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b267e08a-4eed-4b1a-ac17-c91c43205b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/500 H@1=0.320 H@3=0.420 parse_fail=0 empty=0 no_map=0\n",
      "100/500 H@1=0.300 H@3=0.390 parse_fail=0 empty=0 no_map=2\n",
      "150/500 H@1=0.287 H@3=0.387 parse_fail=1 empty=0 no_map=3\n",
      "200/500 H@1=0.300 H@3=0.395 parse_fail=1 empty=0 no_map=5\n",
      "250/500 H@1=0.288 H@3=0.380 parse_fail=1 empty=0 no_map=9\n",
      "300/500 H@1=0.297 H@3=0.380 parse_fail=1 empty=0 no_map=11\n",
      "350/500 H@1=0.300 H@3=0.380 parse_fail=2 empty=0 no_map=11\n",
      "400/500 H@1=0.295 H@3=0.372 parse_fail=2 empty=0 no_map=15\n",
      "450/500 H@1=0.302 H@3=0.378 parse_fail=3 empty=0 no_map=16\n",
      "500/500 H@1=0.316 H@3=0.388 parse_fail=3 empty=0 no_map=18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hits@1': 0.316,\n",
       " 'hits@3': 0.388,\n",
       " 'total': 500,\n",
       " 'parse_fail': 3,\n",
       " 'empty': 0,\n",
       " 'no_map': 18}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = eval_open_world_hits_1_3(df=sample_df, rag_answer=rag_answer_open_world_top3, max_examples=len(sample_df), log_every=50)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebca0d-b973-419a-8d00-594f7c0f52c1",
   "metadata": {},
   "source": [
    "## 3. Cached evaluation on full test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5e70e-eb8f-408b-aa6f-106fd4a0b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "def count_lines(path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def append_jsonl(path, obj):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def eval_checkpoint_generic(\n",
    "    df,\n",
    "    rag_answer,\n",
    "    out_jsonl=\"runs/eval_results.jsonl\",\n",
    "    k_retrieval=24,\n",
    "    max_examples=None,\n",
    "    start_from=\"auto\",\n",
    "    log_every=100,\n",
    "    debug_first_n=3,\n",
    "    store_raw=False,  \n",
    "):\n",
    "    Path(os.path.dirname(out_jsonl) or \".\").mkdir(parents=True, exist_ok=True)\n",
    "    N = len(df) if max_examples is None else min(len(df), max_examples)\n",
    "\n",
    "    start_i = count_lines(out_jsonl) if start_from == \"auto\" else int(start_from)\n",
    "    print(f\"Will process [{start_i} .. {N-1}] (N={N}). Output -> {out_jsonl}\")\n",
    "\n",
    "    debug_printed = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(start_i, N):\n",
    "        row = df.iloc[i]\n",
    "        ts = int(row[\"ts\"])\n",
    "        head_id = str(row[\"head\"])\n",
    "        rel_id  = str(row[\"relation_type\"])\n",
    "        gold_tail = str(row[\"tail\"])\n",
    "\n",
    "        orig_idx = int(row[\"orig_idx\"])\n",
    "\n",
    "        # --- model call ---\n",
    "        pred_labels, retrieved_facts, raw = rag_answer(head_id, rel_id, ts, k=k_retrieval)\n",
    "        # If rag_answer returns raw in format tail_label (single), not tail_labels,\n",
    "        # we parse it here:\n",
    "        if pred_labels is None:\n",
    "            pred_labels = extract_tail_labels_topk(raw, k=3)\n",
    "\n",
    "        status = \"ok\"\n",
    "        if pred_labels is None:\n",
    "            status = \"parse_fail\"\n",
    "        elif len(pred_labels) == 0:\n",
    "            status = \"empty\"\n",
    "\n",
    "        if status == \"parse_fail\" and debug_printed < debug_first_n:\n",
    "            print(\"RAW (parse_fail example):\\n\", raw)\n",
    "            debug_printed += 1\n",
    "\n",
    "        rec = {\n",
    "            \"i\": i,\n",
    "            \"orig_idx\": orig_idx,\n",
    "            \"ts\": ts,\n",
    "            \"head\": head_id,\n",
    "            \"rel\": rel_id,\n",
    "            \"gold_tail\": gold_tail,\n",
    "            \"pred_labels\": pred_labels,   # None / [] / [\"a\",\"b\",\"c\"]\n",
    "            \"status\": status,\n",
    "            \"k\": k_retrieval,\n",
    "        }\n",
    "        if store_raw:\n",
    "            rec[\"raw\"] = raw\n",
    "\n",
    "        append_jsonl(out_jsonl, rec)\n",
    "\n",
    "        if log_every and (i + 1) % log_every == 0:\n",
    "            dt = time.time() - t0\n",
    "            speed = (i + 1 - start_i) / max(dt, 1e-9)\n",
    "            eta = (N - (i + 1)) / max(speed, 1e-9)\n",
    "            print(f\"{i+1}/{N} saved | speed={speed:.3f} ex/s | ETA~{eta/60:.1f} min\")\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08ec2e-13e9-40cf-8add-94a4b7fff7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_checkpoint_generic(\n",
    "    test_df,                        # full test\n",
    "    rag_answer=rag_answer_hybrid_top3,\n",
    "    out_jsonl=\"runs/hybrid_k24.jsonl\",\n",
    "    k_retrieval=24,\n",
    "    start_from=\"auto\",\n",
    "    log_every=100,\n",
    "    store_raw=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e46b88-6108-4a62-be86-19b2aaf07238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_jsonl(path):\n",
    "    h1 = 0\n",
    "    h3 = 0\n",
    "    total = 0\n",
    "    parse_fail = 0\n",
    "    empty = 0\n",
    "    no_map = 0\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            gold_tail = str(rec[\"gold_tail\"])\n",
    "            pred_labels = rec.get(\"pred_labels\", None)\n",
    "            status = rec.get(\"status\", None)\n",
    "\n",
    "            total += 1\n",
    "\n",
    "            if status == \"parse_fail\" or pred_labels is None:\n",
    "                parse_fail += 1\n",
    "                continue\n",
    "            if status == \"empty\" or len(pred_labels) == 0:\n",
    "                empty += 1\n",
    "                continue\n",
    "\n",
    "            # mapping label -> QIDs\n",
    "            qids_1 = tail_label_to_qids(pred_labels[0])\n",
    "            qids_3 = set()\n",
    "            for lab in pred_labels[:3]:\n",
    "                qids_3.update(tail_label_to_qids(lab))\n",
    "\n",
    "            if (not qids_1) and (not qids_3):\n",
    "                no_map += 1\n",
    "\n",
    "            if gold_tail in qids_1:\n",
    "                h1 += 1\n",
    "            if gold_tail in qids_3:\n",
    "                h3 += 1\n",
    "\n",
    "    return {\n",
    "        \"hits@1\": h1 / max(total, 1),\n",
    "        \"hits@3\": h3 / max(total, 1),\n",
    "        \"total\": total,\n",
    "        \"parse_fail\": parse_fail,\n",
    "        \"empty\": empty,\n",
    "        \"no_map\": no_map\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tkgc_rag)",
   "language": "python",
   "name": "tkgc_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
